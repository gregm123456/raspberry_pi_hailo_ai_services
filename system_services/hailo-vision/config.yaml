server:
  host: 0.0.0.0
  port: 11435

model:
  # Model name and variant
  name: "qwen2-vl-2b-instruct"
  
  # Optional: HEF model path override
  # If not specified, will auto-resolve and download via hailo-apps
  # Options:
  #   - null: auto-resolve default model for VLM_CHAT_APP
  #   - "model_name": resolve by name (e.g., "Qwen2-VL-2B-Instruct")
  #   - "/path/to/model.hef": use explicit path
  hef_path: null
  
  # keep_alive controls model lifecycle:
  # -1 = persistent (keep loaded indefinitely)
  # 0 = unload immediately after request
  # N = unload after N seconds (e.g., 300 = 5 minutes)
  # Default if omitted: 300
  keep_alive: -1

generation:
  # Default inference parameters
  temperature: 0.7
  max_tokens: 200
  top_p: 0.9
  seed: null  # null = random, set to integer for reproducibility

# Optional resource limits (tunable)
resource_limits:
  # systemd unit memory cap
  memory_max: "4G"
  # systemd unit CPU quota (80% = can use 4 out of 5 CPU cores on Pi 5)
  cpu_quota: "80%"
